import{_ as e,c as i,a4 as s,o as t}from"./chunks/framework.su6J3y5l.js";const n="/tech_insight/assets/fig1-2.ZwNhtbm2.png",p="/tech_insight/assets/fig1-4.DxIpS-ZB.png",k=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"arch/ddia/ch1.md","filePath":"arch/ddia/ch1.md","lastUpdated":1732976097000}'),r={name:"arch/ddia/ch1.md"};function l(o,a,h,d,c,m){return t(),i("div",null,a[0]||(a[0]=[s(`<p>人们经常追求这些词汇，却没有清楚理解它们到底意味着什么。</p><blockquote><p>互联网做得太棒了，以至于大多数人将它看作像太平洋这样的自然资源，而不是什么人工产物。上一次出现这种大规模且无差错的技术，你还记得是什么时候吗？</p><p>—— <a href="http://www.drdobbs.com/architecture-and-design/interview-with-alan-kay/240003442" target="_blank" rel="noreferrer">艾伦・凯</a> 在接受 Dobb 博士杂志采访时说（2012 年）</p></blockquote><hr><p>现今很多应用程序都是 <strong>数据密集型（data-intensive）</strong> 的，而非 <strong>计算密集型（compute-intensive）</strong> 的。因此 CPU 很少成为这类应用的瓶颈，更大的问题通常来自数据量、数据复杂性、以及数据的变更速度。</p><p>数据密集型应用通常由标准组件构建而成，标准组件提供了很多通用的功能；例如，许多应用程序都需要：</p><ul><li>存储数据，以便自己或其他应用程序之后能再次找到 （<em>数据库，即 databases</em>）</li><li>记住开销昂贵操作的结果，加快读取速度（<em>缓存，即 caches</em>）</li><li>允许用户按关键字搜索数据，或以各种方式对数据进行过滤（<em>搜索索引，即 search indexes</em>）</li><li>向其他进程发送消息，进行异步处理（<em>流处理，即 stream processing</em>）</li><li>定期处理累积的大批量数据（<em>批处理，即 batch processing</em>）</li></ul><h2 id="可伸缩性" tabindex="-1">可伸缩性 <a class="header-anchor" href="#可伸缩性" aria-label="Permalink to &quot;可伸缩性&quot;">​</a></h2><h3 id="描述负载" tabindex="-1">描述负载 <a class="header-anchor" href="#描述负载" aria-label="Permalink to &quot;描述负载&quot;">​</a></h3><p>推特的两个主要业务是：</p><ul><li><p>发布推文</p><p>用户可以向其粉丝发布新消息（平均 4.6k 请求 / 秒，峰值超过 12k 请求 / 秒）。</p></li><li><p>主页时间线</p><p>用户可以查阅他们关注的人发布的推文（300k 请求 / 秒）。</p></li></ul><p>大体上讲，这一对操作有两种实现方式。 1.当一个用户请求自己的主页时间线时，首先查找他关注的所有人，查询这些被关注用户发布的推文并按时间顺序合并</p><div class="language-sql vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">sql</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">SELECT</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">WHERE</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> follows</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">follower_id</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> current_user</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><img src="`+n+'" alt=""></p><p>2.为每个用户的主页时间线维护一个缓存，就像每个用户的推文收件箱（图 1-3）。当一个用户发布推文时，查找所有关注该用户的人，并将新的推文插入到每个主页时间线缓存中。因此读取主页时间线的请求开销很小，因为结果已经提前计算好了</p><p>因为发推频率比查询主页时间线的频率几乎低了两个数量级，所以在这种情况下，最好在写入时做更多的工作，而在读取时做更少的工作。 因为发推频率比查询主页时间线的频率几乎低了两个数量级，所以在这种情况下，最好在写入时做更多的工作，而在读取时做更少的工作。</p><h3 id="描述性能" tabindex="-1">描述性能 <a class="header-anchor" href="#描述性能" aria-label="Permalink to &quot;描述性能&quot;">​</a></h3><p>Hadoop 吞吐量（throughput） 响应时间（response time），即客户端发送请求到接收响应之间的时间</p><blockquote><h4 id="延迟和响应时间" tabindex="-1">延迟和响应时间 <a class="header-anchor" href="#延迟和响应时间" aria-label="Permalink to &quot;延迟和响应时间&quot;">​</a></h4><p>延迟（latency） 和 响应时间（response time） 经常用作同义词，但实际上它们并不一样。响应时间是客户所看到的，除了实际处理请求的时间（ 服务时间（service time） ）之外，还包括网络延迟和排队延迟。延迟是某个请求等待处理的 持续时长，在此期间它处于 休眠（latent） 状态，并等待服务【17】。</p></blockquote><p>即使不断重复发送同样的请求，每次得到的响应时间也都会略有不同。现实世界的系统会处理各式各样的请求，响应时间可能会有很大差异。因此我们需要将响应时间视为一个可以测量的数值 分布（distribution），而不是单个数值。</p><p>在 <a href="imgs/fig1-4.png">图 1-4</a> 中，每个灰条代表一次对服务的请求，其高度表示请求花费了多长时间</p><p><img src="'+p+'" alt=""></p><p><strong>图 1-4 展示了一个服务 100 次请求响应时间的均值与百分位数</strong></p><p>通常报表都会展示服务的平均响应时间。（严格来讲 “平均” 一词并不指代任何特定公式，但实际上它通常被理解为 算术平均值（arithmetic mean）：给定 n 个值，加起来除以 n ）。然而如果你想知道 “典型（typical）” 响应时间，那么平均值并不是一个非常好的指标，因为它不能告诉你有多少用户实际上经历了这个延迟。</p><p>通常使用 百分位点（percentiles） 会更好。如果将响应时间列表按最快到最慢排序，那么 中位数（median） 就在正中间：举个例子，如果你的响应时间中位数是 200 毫秒，这意味着一半请求的返回时间少于 200 毫秒，另一半比这个要长。</p><p>如果想知道典型场景下用户需要等待多长时间，那么中位数是一个好的度量标准：一半用户请求的响应时间少于响应时间的中位数，另一半服务时间比中位数长。中位数也被称为第 50 百分位点，有时缩写为 p50。注意中位数是关于单个请求的；如果用户同时发出几个请求（在一个会话过程中，或者由于一个页面中包含了多个资源），则至少一个请求比中位数慢的概率远大于 50%。</p><p>为了弄清异常值有多糟糕，可以看看更高的百分位点，例如第 95、99 和 99.9 百分位点（缩写为 p95，p99 和 p999）。它们意味着 95%、99% 或 99.9% 的请求响应时间要比该阈值快，例如：如果第 95 百分位点响应时间是 1.5 秒，则意味着 100 个请求中的 95 个响应时间快于 1.5 秒，而 100 个请求中的 5 个响应时间超过 1.5 秒。</p><p>响应时间的高百分位点（也称为 尾部延迟，即 tail latencies）非常重要，因为它们直接影响用户的服务体验。例如亚马逊在描述内部服务的响应时间要求时是以 99.9 百分位点为准，即使它只影响一千个请求中的一个。这是因为请求响应最慢的客户往往也是数据最多的客户，也可以说是最有价值的客户 —— 因为他们掏钱了[^19]。保证网站响应迅速对于保持客户的满意度非常重要，亚马逊观察到：响应时间增加 100 毫秒，销售量就减少 1%【20】；而另一些报告说：慢 1 秒钟会让客户满意度指标减少 16%【21，22】</p><p>另一方面，优化第 99.99 百分位点（一万个请求中最慢的一个）被认为太昂贵了，不能为亚马逊的目标带来足够好处。减小高百分位点处的响应时间相当困难，因为它很容易受到随机事件的影响，这超出了控制范围，而且效益也很小。</p><p>服务级别协议（SLA, service level agreements，即定义服务预期性能和可用性的合同。SLA 可能会声明，如果服务响应时间的中位数小于 200 毫秒，且 99.9 百分位点低于 1 秒，则认为服务工作正常（如果响应时间更长，就认为服务不达标）。这些指标为客户设定了期望值，并允许客户在 SLA 未达标的情况下要求退款。</p><p>排队延迟（queueing delay） 通常占了高百分位点处响应时间的很大一部分。由于服务器只能并行处理少量的事务（如受其 CPU 核数的限制），所以只要有少量缓慢的请求就能阻碍后续请求的处理，这种效应有时被称为 头部阻塞（head-of-line blocking） 。即使后续请求在服务器上处理的非常迅速，由于需要等待先前请求完成，客户端最终看到的是缓慢的总体响应时间。因为存在这种效应，测量客户端的响应时间非常重要</p><h3 id="应对负载的方法" tabindex="-1">应对负载的方法 <a class="header-anchor" href="#应对负载的方法" aria-label="Permalink to &quot;应对负载的方法&quot;">​</a></h3><h2 id="参考资料" tabindex="-1">参考资料 <a class="header-anchor" href="#参考资料" aria-label="Permalink to &quot;参考资料&quot;">​</a></h2><p>[^19]: Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: “<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" target="_blank" rel="noreferrer">Dynamo: Amazon&#39;s Highly Available Key-Value Store</a>,” at <em>21st ACM Symposium on Operating Systems Principles</em> (SOSP), October 2007. [^19]:</p>',33)]))}const g=e(r,[["render",l]]);export{k as __pageData,g as default};
